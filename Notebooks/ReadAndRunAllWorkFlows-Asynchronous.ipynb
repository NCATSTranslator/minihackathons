{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Query Status in CSV for all Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Read all the JSON files for all the workflows and print out the messages and query status to a CSV file**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules. NB: submit_run_ars_modules contains all the modules to submit job to ARAX\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from gamma_viewer import GammaViewer\n",
    "from IPython.display import display\n",
    "#from submit_run_ars_modules import submit_to_ars, submit_to_devars, printjson, retrieve_devars_results\n",
    "import glob \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_ars(m,ars_url='https://ars-dev.transltr.io/ars/api',arax_url='https://arax.ncats.io'):\n",
    "    submit_url=f'{ars_url}/submit'\n",
    "    response = requests.post(submit_url,json=m)\n",
    "    try:\n",
    "        message_id = response.json()['pk']\n",
    "    except:\n",
    "        print('fail')\n",
    "        message_id = None\n",
    "    print(f'{arax_url}/?source=ARS&id={message_id}')\n",
    "    return message_id\n",
    "\n",
    "##https://ars.ci.transltr.io/ars/api\n",
    "\n",
    "def retrieve_ars_results(mid,ars_url='https://ars-dev.transltr.io/ars/api'):\n",
    "    pk = 'https://arax.ncats.io/?source=ARS&id=' + mid\n",
    "    message_url = f'{ars_url}/messages/{mid}?trace=y'\n",
    "    response = requests.get(message_url)\n",
    "    j = response.json()\n",
    "    print( j['status'] )\n",
    "    results = {}\n",
    "    dictionary = {}\n",
    "    dictionary_2 = {}\n",
    "    for child in j['children']:\n",
    "        print(child['status'])\n",
    "        error_code = child['code']\n",
    "        \n",
    "        if child['status']  == 'Done':\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                nresults = len(child_response['fields']['data']['message']['results'])\n",
    "                if nresults > 0:\n",
    "                    results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                    \n",
    "                if child_response['fields']['data']['message']['knowledge_graph']['edges']:\n",
    "                    if child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                            edge_ex = child_response['fields']['data']['message']['knowledge_graph']['edges']\n",
    "                            test_att_values =[]\n",
    "                            for val in child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                                #print(val)\n",
    "                                \n",
    "                                for tx in edge_ex[val]['attributes']:\n",
    "                                    if (tx['attribute_type_id'] == 'biolink:primary_knowledge_source') or (tx['attribute_type_id'] == 'biolink:original_knowledge_source') or (tx['attribute_type_id'] == 'biolink:aggregator_knowledge_source') :\n",
    "                                        \n",
    "                                        \n",
    "                                        value_att = tx['value']\n",
    "                        \n",
    "                                        test_att_values.append(value_att)\n",
    "                                        test_att = set(flatten_list(test_att_values))\n",
    "                                        \n",
    "                                        \n",
    "                                        dictionary_2[child['actor']['agent']] = test_att\n",
    "                    #else:\n",
    "                        #dictionary_2[child['actor']['agent']] = [] \n",
    "                #else:\n",
    "                   # dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "            except Exception as e:\n",
    "                nresults=0\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "                \n",
    "            \n",
    "        \n",
    "        elif child['status'] == 'Error':\n",
    "            nresults=0\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            nresults = 0\n",
    "            #dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "        dictionary['pk_id'] =  pk  \n",
    "            \n",
    "        if ((child['status'] == 'Done') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'No Results' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'No Results']\n",
    "        elif ((child['status'] == 'ARS Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'ARS Error' ': ' + str(error_code)\n",
    "        elif ((child['status'] == 'Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'Error' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'ARS Error']\n",
    "        elif ((child['status'] == 'Done') & (nresults != 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Results' ': ' + str(error_code)\n",
    "        elif ((child['status'] == 'Unknown') & (nresults == 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Unknown' ': ' + str(error_code)\n",
    "        \n",
    "        \n",
    "        print(child['actor']['agent'], child['status'], nresults)\n",
    "        #test =  [child['actor']['agent'], child['status'], nresults]\n",
    "        #test2.append(test)\n",
    "    return [dictionary, dictionary_2]\n",
    "\n",
    "\n",
    "#def submit_to_devars(m):\n",
    "#    return submit_to_ars(m,ars_url='https://ars-dev.transltr.io/ars/api',arax_url='https://arax.ncats.io')\n",
    "\n",
    "#def retrieve_devars_results(m):\n",
    "#     return retrieve_ars_results(m,ars_url='https://ars-dev.transltr.io/ars/api')\n",
    "\n",
    "def printjson(j):\n",
    "    print(json.dumps(j,indent=4))\n",
    "    \n",
    "def make_hyperlink(value):\n",
    "    return '=HYPERLINK(\"%s\", \"%s\")' % (value.format(value), value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The below code reads each JSON files from the Workflows A through D (subdirectories). The queries are submitted to ARAX and output is saved in a dictionary, where the key is the file name of the JSON to denote which query is being run and the values assigned to the key is the query id**\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATH = r'/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo'\n",
    "EXT = \"*.json\"\n",
    "dict_workflows = {}\n",
    "for root, dirs, files in os.walk(PATH): # step 1: accessing file\n",
    "    #print(root)\n",
    "    for name in files:\n",
    "        \n",
    "        if name.endswith((\".json\")):\n",
    "            file_read = path.join(root, name)\n",
    "            dir_name = (os.path.splitext(os.path.basename(root))[0])\n",
    "            print(file_read)\n",
    "            \n",
    "            filename = (os.path.splitext(os.path.basename(file_read))[0])\n",
    "            print(filename)\n",
    "            with open(file_read,'r') as inf:\n",
    "                query = json.load(inf)\n",
    "                \n",
    "                kcresult = submit_to_ars(query)\n",
    "                \n",
    "                sleep(300)\n",
    "                \n",
    "                result_status = retrieve_ars_results(kcresult)\n",
    "                \n",
    "        \n",
    "                dict_workflows[filename] = kcresult\n",
    "                            \n",
    "                sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Codes below are for recording messages and generating outout as csv\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages = {}\n",
    "for keys, val in dict_workflows.items():\n",
    "    print(keys, val)\n",
    "    \n",
    "    result_status = retrieve_ars_results(val)\n",
    "    \n",
    "    workflow_result_messages[keys] = result_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe for workflows with PK\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert mesages to a dataframe\n",
    "col = []\n",
    "final_dict = defaultdict(list)\n",
    "\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    col.append(k)\n",
    "    \n",
    "    for key, value in workflow_result_messages[k][0].items():\n",
    "        #print(key, value)\n",
    "#         if key.startswith('kp-'):\n",
    "#             key_mod = key.replace('kp-','')\n",
    "#         else:\n",
    "#             key_mod = key\n",
    "        \n",
    "        final_dict[key].append(value)\n",
    "\n",
    "    final_dict = dict(final_dict)\n",
    "    \n",
    "df = pd.DataFrame(final_dict).T\n",
    "df.rename(columns=dict(zip(df.columns, col)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('ARS Error', 'No Results', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating second table with edge attribute source\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict2 = defaultdict(dict)\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    col.append(k)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for key, value in workflow_result_messages[k][1].items():\n",
    "        final_dict2[k][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble = []\n",
    "for k, vs in final_dict2.items():\n",
    "    #print(k,vs)\n",
    "    for kv, v in vs.items():\n",
    "        for t in v:\n",
    "            final_dictassemble.append([k,kv,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Workflow', 'ARS-KPs', 'Values']\n",
    "df2 = pd.DataFrame(final_dictassemble, columns=column_names)\n",
    "df2 = df2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Values = df2.Values.apply(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2.groupby(['Workflow','Values'])['ARS-KPs'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = pd.DataFrame(df2test.unstack().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.drop([''], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.index = df2test.index.map(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.replace([], 'None', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test = df2test.mask(df2test.applymap(type).eq(list) & ~df2test.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/InfoRes Catalog - Translator InfoRes Catalog.csv\", header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[['id', 'name','translator category','has contributor']]\n",
    "infores_catalog = infores_catalog[:335]\n",
    "dict_map = {}\n",
    "for i in df2test.index.values:\n",
    "    if i in infores_catalog['id'].values:\n",
    "        indices = infores_catalog[infores_catalog['id']==i].index[0]\n",
    "        if pd.notnull(infores_catalog.iloc[indices]['has contributor']):\n",
    "            dict_map[i] = infores_catalog.iloc[indices]['translator category']\n",
    "        else:\n",
    "            dict_map[i] = 'External Source'\n",
    "    else:\n",
    "        dict_map[i] = 'Illegal value'\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog']=df2test.index.map(dict_map)\n",
    "df2test['sort']=pd.Categorical(df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog'], [\"KP\", \"ARA\",'External Source', 'Illegal Value'])\n",
    "df2test =df2test.sort_values(['sort'])\n",
    "df2test  = df2test.rename_axis(None)\n",
    "cols = df2test.columns.tolist()\n",
    "cols = [cols[-2]] + cols[:-2]\n",
    "df2test = df2test[cols]\n",
    "df2test['Query Type'] = 'Async'\n",
    "\n",
    "df2test = df2test[['Query Type']+ list(df2test.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Converting the Pk's to hyperlink\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['pk_id'] = df.loc['pk_id'].apply(lambda x: make_hyperlink(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'pk_id': 'pk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Query Type'] = 'Async'\n",
    "\n",
    "df = df[['Query Type']+ list(df.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "wks_name = 'Workflow Progress Tracker Asynchronous_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wks_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pushing dataframe to excel sheet on google drive\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Here I am using the google drive API to push the daatframe into an axcel sheet \n",
    "Every individula has the unique credential file that they need to create for google drive API -- \n",
    "\"araxworkflowprogresstesting-2632632db8be.json\" -- is the credential used from my drive. place this json file where\n",
    "the ReadAndRunAllWorkFLows.ipynb will be. NB: i have removed my credntial file for privacy reasons. Always remove\n",
    "the json file before making committs to the repo. To use googe Drive API follow: https://towardsdatascience.com/how-to-manage-files-in-google-drive-with-python-d26471d91ecd**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Google Drive API\n",
    "\n",
    "**Push the dataframe to a google sheet via google drive API and then format the google spread sheet to add hyperlink to pk's and color the cells\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 1\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from gspread_formatting import *\n",
    "import gspread_dataframe as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    'araxworkflowprogresstesting-2632632db8be.json', scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = gc.open(\"workflow_progress_tracker\").worksheet(\"Workflow Progress Tracker_2021_09_16-12_17_56_AM\")\n",
    "\n",
    "#existing = gd.get_as_dataframe(ws)\n",
    "existing = pd.DataFrame(ws.get_all_records())\n",
    "\n",
    "existing.set_index('', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "existing = existing.rename_axis(None)\n",
    "\n",
    "existing.loc['pk'][:] = existing.loc['pk'][:].apply(lambda x: x.replace('\" \"', '\",\"'))\n",
    "\n",
    "existing.loc['pk'][1]\n",
    "\n",
    "updated = existing.append(df)\n",
    "gd.set_with_dataframe(ws, updated,include_index=True,include_column_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "#d2g.upload(df, spreadsheet_key, \"TESTING ONLY\", credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = gspread.service_account(filename='/Users/priyash/Documents/GitHub/minihackathons/Notebooks/araxworkflowprogresstesting-2632632db8be.json')\n",
    "wksh = gc.open(\"workflow_progress_tracker\")\n",
    "#sh = wksh.worksheet(wks_name)\n",
    "sh = wksh.worksheet(\"Workflow Progress Tracker_2021_09_16-12_17_56_AM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ConditionalFormatRule(\n",
    "    ranges=[GridRange.from_a1_range('C2:{}23', sh)],\n",
    "    booleanRule=BooleanRule(\n",
    "        condition=BooleanCondition('TEXT_STARTS_WITH', ['Error']),\n",
    "        format=CellFormat(textFormat=textFormat(bold=True), backgroundColor=Color(1,0,0))\n",
    "    )\n",
    ")\n",
    "rules = get_conditional_format_rules(sh)\n",
    "rules.append(rule)\n",
    "rules.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ConditionalFormatRule(\n",
    "    ranges=[GridRange.from_a1_range('C2:{}23', sh)],\n",
    "    booleanRule=BooleanRule(\n",
    "        condition=BooleanCondition('TEXT_STARTS_WITH', ['Results']),\n",
    "        format=CellFormat(textFormat=textFormat(bold=True), backgroundColor=Color(0.0, 0.5, 0.0))\n",
    "    )\n",
    ")\n",
    "rules = get_conditional_format_rules(sh)\n",
    "rules.append(rule)\n",
    "rules.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ConditionalFormatRule(\n",
    "    ranges=[GridRange.from_a1_range('C2:{}23', sh)],\n",
    "    booleanRule=BooleanRule(\n",
    "        condition=BooleanCondition('TEXT_STARTS_WITH', ['No Results']),\n",
    "        format=CellFormat(textFormat=textFormat(bold=True), backgroundColor=Color(0.75, 0.75, 0))\n",
    "    )\n",
    ")\n",
    "rules = get_conditional_format_rules(sh)\n",
    "rules.append(rule)\n",
    "rules.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ConditionalFormatRule(\n",
    "    ranges=[GridRange.from_a1_range('C2:{}23', sh)],\n",
    "    booleanRule=BooleanRule(\n",
    "        condition=BooleanCondition('TEXT_STARTS_WITH', ['ARS Error']),\n",
    "        format=CellFormat(textFormat=textFormat(bold=True), backgroundColor=Color(0.0, 0.75, 0.75))\n",
    "    )\n",
    ")\n",
    "rules = get_conditional_format_rules(sh)\n",
    "rules.append(rule)\n",
    "rules.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ConditionalFormatRule(\n",
    "    ranges=[GridRange.from_a1_range('C2:{}23', sh)],\n",
    "    booleanRule=BooleanRule(\n",
    "        condition=BooleanCondition('TEXT_STARTS_WITH', ['Unknown']),\n",
    "        format=CellFormat(textFormat=textFormat(bold=True), backgroundColor=Color(0.75, 0, 0.75))\n",
    "    )\n",
    ")\n",
    "rules = get_conditional_format_rules(sh)\n",
    "rules.append(rule)\n",
    "rules.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a range\n",
    "cell_list = sh.range('B19:Y19')\n",
    "\n",
    "# Update in batch\n",
    "sh.update_cells(cell_list,value_input_option='USER_ENTERED')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a range\n",
    "cell_list = sh.range('B23:Y23')\n",
    "\n",
    "# Update in batch\n",
    "sh.update_cells(cell_list,value_input_option='USER_ENTERED')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 2\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws2 = gc.open(\"workflow_progress_tracker\").worksheet(\"edge_attribute_source_2021_09_16-12_17_56_AM\")\n",
    "\n",
    "#existing = gd.get_as_dataframe(ws)\n",
    "existing = pd.DataFrame(ws2.get_all_records())\n",
    "\n",
    "existing.set_index('', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "existing = existing.rename_axis(None)\n",
    "\n",
    "updated = existing.append(df2test)\n",
    "gd.set_with_dataframe(ws2, updated,include_index=True,include_column_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

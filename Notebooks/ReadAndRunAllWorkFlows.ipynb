{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Query Status in CSV for all Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Read all the JSON files for all the workflows and print out the messages and query status to a CSV file**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules. NB: submit_run_ars_modules contains all the modules to submit job to ARAX\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from gamma_viewer import GammaViewer\n",
    "from IPython.display import display\n",
    "#from submit_run_ars_modules import submit_to_ars, submit_to_devars, printjson, retrieve_devars_results\n",
    "import glob \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_ars(m,ars_url='https://ars.transltr.io/ars/api',arax_url='https://arax.ncats.io'):\n",
    "    submit_url=f'{ars_url}/submit'\n",
    "    response = requests.post(submit_url,json=m)\n",
    "    try:\n",
    "        message_id = response.json()['pk']\n",
    "    except:\n",
    "        print('fail')\n",
    "        message_id = None\n",
    "    print(f'{arax_url}/?source=ARS&id={message_id}')\n",
    "    return message_id\n",
    "\n",
    "##https://ars.ci.transltr.io/ars/api\n",
    "\n",
    "def retrieve_ars_results(mid, name, check_sheet, ars_url='https://ars.transltr.io/ars/api'):\n",
    "    pk = 'https://arax.ncats.io/?source=ARS&id=' + mid\n",
    "    message_url = f'{ars_url}/messages/{mid}?trace=y'\n",
    "    response = requests.get(message_url)\n",
    "    j = response.json()\n",
    "    print( j['status'] )\n",
    "    results = {}\n",
    "    dictionary = {}\n",
    "    dictionary_2 = {}\n",
    "    dict3 = {}\n",
    "    for child in j['children']:\n",
    "        print(child['status'])\n",
    "        error_code = child['code']\n",
    "        \n",
    "        if child['status']  == 'Done':\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                nresults = len(child_response['fields']['data']['message']['results'])\n",
    "                if nresults > 0:\n",
    "                    results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                    \n",
    "                    if name in list(check_sheet.Workflow):\n",
    "                        print(name)\n",
    "                        dfy = check_sheet[check_sheet['Workflow']== name]\n",
    "\n",
    "                        dfy.reset_index(drop=True)\n",
    "\n",
    "                        for index,curie_id in enumerate(dfy.Curie):\n",
    "                            print(index,curie_id)\n",
    "                            node_num = dfy.iloc[index][3]\n",
    "                            query_id = curie_id\n",
    "                            if np.isnan(dfy.iloc[index][2]) == True:\n",
    "                                len_check = len(child_response['fields']['data']['message']['results'])\n",
    "                                len_check = int(len_check)\n",
    "                            else:\n",
    "                                len_check = int(dfy.iloc[index][2])\n",
    "\n",
    "                            #print(node_num, query_id, len_check)\n",
    "\n",
    "                            locs = []\n",
    "                            for x, val in enumerate(child_response['fields']['data']['message']['results']):\n",
    "                                #print(val)\n",
    "\n",
    "                                if x < len_check:\n",
    "\n",
    "                                    if query_id in val['node_bindings'][node_num][0]['id']:\n",
    "                                        locs.append(x)\n",
    "                            if not locs:\n",
    "                                check_result = f'False'\n",
    "                                print(check_result)\n",
    "                                #pass\n",
    "                            else:\n",
    "                                check_result = f'True'\n",
    "                                print('curie id:', query_id, ': INCLUDED at postion N ==', locs, 'on', node_num)\n",
    "\n",
    "                            dict3[curie_id] = check_result    \n",
    "\n",
    "                if child_response['fields']['data']['message']['knowledge_graph']['edges']:\n",
    "                    if child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                            edge_ex = child_response['fields']['data']['message']['knowledge_graph']['edges']\n",
    "                            test_att_values =[]\n",
    "                            for val in child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                                #print(val)\n",
    "                                \n",
    "                                for tx in edge_ex[val]['attributes']:\n",
    "                                    if (tx['attribute_type_id'] == 'biolink:primary_knowledge_source') or (tx['attribute_type_id'] == 'biolink:original_knowledge_source') or (tx['attribute_type_id'] == 'biolink:aggregator_knowledge_source') :\n",
    "                                        \n",
    "                                        \n",
    "                                        value_att = tx['value']\n",
    "                        \n",
    "                                        test_att_values.append(value_att)\n",
    "                                        test_att = set(flatten_list(test_att_values))\n",
    "                                        \n",
    "                                        \n",
    "                                        dictionary_2[child['actor']['agent']] = test_att\n",
    "                    #else:\n",
    "                        #dictionary_2[child['actor']['agent']] = [] \n",
    "                #else:\n",
    "                   # dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "            except Exception as e:\n",
    "                nresults=0\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "                \n",
    "            \n",
    "        \n",
    "        elif child['status'] == 'Error':\n",
    "            nresults=0\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            nresults = 0\n",
    "            #dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "        dictionary['pk_id'] =  pk  \n",
    "            \n",
    "        if ((child['status'] == 'Done') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'No Results' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'No Results']\n",
    "        elif ((child['status'] == 'ARS Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'ARS Error' ': ' + str(error_code)\n",
    "        elif ((child['status'] == 'Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'Error' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'ARS Error']\n",
    "        elif ((child['status'] == 'Done') & (nresults != 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Results' ': ' + str(error_code) + ' ' + str(dict3)\n",
    "        elif ((child['status'] == 'Unknown') & (nresults == 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Unknown' ': ' + str(error_code)\n",
    "        \n",
    "        \n",
    "        print(child['actor']['agent'], child['status'], nresults)\n",
    "        #test =  [child['actor']['agent'], child['status'], nresults]\n",
    "        #test2.append(test)\n",
    "    return [dictionary, dictionary_2]\n",
    "\n",
    "\n",
    "#def submit_to_devars(m):\n",
    "#    return submit_to_ars(m,ars_url='https://ars-dev.transltr.io/ars/api',arax_url='https://arax.ncats.io')\n",
    "\n",
    "#def retrieve_devars_results(m):\n",
    "#     return retrieve_ars_results(m,ars_url='https://ars-dev.transltr.io/ars/api')\n",
    "\n",
    "def printjson(j):\n",
    "    print(json.dumps(j,indent=4))\n",
    "    \n",
    "def make_hyperlink(value):\n",
    "    return '=HYPERLINK(\"%s\", \"%s\")' % (value.format(value), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sheet = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/Query results to include or exclude - Sheet1.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Workflow</th>\n",
       "      <th>Curie</th>\n",
       "      <th>N (size of list of results)</th>\n",
       "      <th>Query node ID</th>\n",
       "      <th>Include/Exclude</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.0_RHOBTB2_direct.json</td>\n",
       "      <td>PUBCHEM.COMPOUND:2950270</td>\n",
       "      <td>10.0</td>\n",
       "      <td>n1</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Note: this example row means that for workflow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>CHEBI:41879</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>MESH:D000077185</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>MESH:D000077385</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>MESH:D003474</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>RXNORM:40068</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>PUBCHEM.COMPOUND:5877</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B.1_DILI-three-hop-interacts-with.json</td>\n",
       "      <td>PUBCHEM.COMPOUND:5755</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C.1a_SmallMolecule_real_world_evidence_MultScl...</td>\n",
       "      <td>MONDO:0005301</td>\n",
       "      <td>100.0</td>\n",
       "      <td>n00</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C.2a_Imatinib_MultSclerosis_GeneSet_and_SmallM...</td>\n",
       "      <td>CHEBI:45783</td>\n",
       "      <td>100.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C.2b_Etanercept_MultSclerosis_GeneSet_and_Smal...</td>\n",
       "      <td>CHEBI:4875</td>\n",
       "      <td>100.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C.2c_Natalizumab_MultSclerosis_GeneSet_and_Sma...</td>\n",
       "      <td>CHEMBL.COMPOUND:CHEMBL1201607</td>\n",
       "      <td>100.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D.1_parkinsons-crohns.json</td>\n",
       "      <td>NCBIGene:120892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LRRK2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D.1_parkinsons-crohns.json</td>\n",
       "      <td>NCBIGene:11315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PARK7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D.1_parkinsons-crohns.json</td>\n",
       "      <td>NCBIGene:110357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOD2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D.2_ssri-heart-disease.json</td>\n",
       "      <td>NCBIGene:3988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D.2_ssri-heart-disease.json</td>\n",
       "      <td>NCBIGene:5627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D.2_ssri-heart-disease.json</td>\n",
       "      <td>NCBIGene:7043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TGFB3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>REACT:R-HSA-888614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n02</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>KEGG:1.13.11.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n02</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>REACT:R-HSA-888614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n03</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>KEGG:1.13.11.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n03</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D.6_metformin-ferritin.json</td>\n",
       "      <td>UniProtKB:P54646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPK protein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D.6_metformin-ferritin.json</td>\n",
       "      <td>NCBIGene:5563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMPK gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D.6_metformin-ferritin.json</td>\n",
       "      <td>UniProtKB:P42345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n02</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MTOR protein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Workflow  \\\n",
       "0                             A.0_RHOBTB2_direct.json   \n",
       "1              B.1_DILI-three-hop-interacts-with.json   \n",
       "2              B.1_DILI-three-hop-interacts-with.json   \n",
       "3              B.1_DILI-three-hop-interacts-with.json   \n",
       "4              B.1_DILI-three-hop-interacts-with.json   \n",
       "5              B.1_DILI-three-hop-interacts-with.json   \n",
       "6              B.1_DILI-three-hop-interacts-with.json   \n",
       "7              B.1_DILI-three-hop-interacts-with.json   \n",
       "8   C.1a_SmallMolecule_real_world_evidence_MultScl...   \n",
       "9   C.2a_Imatinib_MultSclerosis_GeneSet_and_SmallM...   \n",
       "10  C.2b_Etanercept_MultSclerosis_GeneSet_and_Smal...   \n",
       "11  C.2c_Natalizumab_MultSclerosis_GeneSet_and_Sma...   \n",
       "12                         D.1_parkinsons-crohns.json   \n",
       "13                         D.1_parkinsons-crohns.json   \n",
       "14                         D.1_parkinsons-crohns.json   \n",
       "15                        D.2_ssri-heart-disease.json   \n",
       "16                        D.2_ssri-heart-disease.json   \n",
       "17                        D.2_ssri-heart-disease.json   \n",
       "18                      D.4_tryptophan-kyurenine.json   \n",
       "19                      D.4_tryptophan-kyurenine.json   \n",
       "20                      D.4_tryptophan-kyurenine.json   \n",
       "21                      D.4_tryptophan-kyurenine.json   \n",
       "22                        D.6_metformin-ferritin.json   \n",
       "23                        D.6_metformin-ferritin.json   \n",
       "24                        D.6_metformin-ferritin.json   \n",
       "\n",
       "                            Curie  N (size of list of results) Query node ID  \\\n",
       "0        PUBCHEM.COMPOUND:2950270                         10.0            n1   \n",
       "1                     CHEBI:41879                        500.0            n3   \n",
       "2                 MESH:D000077185                        500.0            n3   \n",
       "3                 MESH:D000077385                        500.0            n3   \n",
       "4                    MESH:D003474                        500.0            n3   \n",
       "5                    RXNORM:40068                        500.0            n3   \n",
       "6           PUBCHEM.COMPOUND:5877                        500.0            n3   \n",
       "7           PUBCHEM.COMPOUND:5755                        500.0            n3   \n",
       "8                   MONDO:0005301                        100.0           n00   \n",
       "9                     CHEBI:45783                        100.0            n3   \n",
       "10                     CHEBI:4875                        100.0            n3   \n",
       "11  CHEMBL.COMPOUND:CHEMBL1201607                        100.0            n3   \n",
       "12                NCBIGene:120892                          NaN           n01   \n",
       "13                 NCBIGene:11315                          NaN           n01   \n",
       "14                NCBIGene:110357                          NaN           n01   \n",
       "15                  NCBIGene:3988                          NaN           n01   \n",
       "16                  NCBIGene:5627                          NaN           n01   \n",
       "17                  NCBIGene:7043                          NaN           n01   \n",
       "18             REACT:R-HSA-888614                          NaN           n02   \n",
       "19                KEGG:1.13.11.52                          NaN           n02   \n",
       "20             REACT:R-HSA-888614                          NaN           n03   \n",
       "21                KEGG:1.13.11.52                          NaN           n03   \n",
       "22               UniProtKB:P54646                          NaN           n01   \n",
       "23                  NCBIGene:5563                          NaN           n01   \n",
       "24               UniProtKB:P42345                          NaN           n02   \n",
       "\n",
       "   Include/Exclude  Unnamed: 5  \\\n",
       "0          Include         NaN   \n",
       "1          Include         NaN   \n",
       "2          Include         NaN   \n",
       "3          Include         NaN   \n",
       "4          Include         NaN   \n",
       "5          Include         NaN   \n",
       "6          Include         NaN   \n",
       "7          Include         NaN   \n",
       "8          include         NaN   \n",
       "9          include         NaN   \n",
       "10         include         NaN   \n",
       "11         include         NaN   \n",
       "12         include         NaN   \n",
       "13         include         NaN   \n",
       "14         include         NaN   \n",
       "15         include         NaN   \n",
       "16         include         NaN   \n",
       "17         include         NaN   \n",
       "18         include         NaN   \n",
       "19         include         NaN   \n",
       "20         include         NaN   \n",
       "21         include         NaN   \n",
       "22         include         NaN   \n",
       "23         include         NaN   \n",
       "24         include         NaN   \n",
       "\n",
       "                                           Unnamed: 6  \n",
       "0   Note: this example row means that for workflow...  \n",
       "1       Equivalent identifiers are equally acceptable  \n",
       "2       Equivalent identifiers are equally acceptable  \n",
       "3       Equivalent identifiers are equally acceptable  \n",
       "4       Equivalent identifiers are equally acceptable  \n",
       "5       Equivalent identifiers are equally acceptable  \n",
       "6       Equivalent identifiers are equally acceptable  \n",
       "7       Equivalent identifiers are equally acceptable  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                              LRRK2  \n",
       "13                                              PARK7  \n",
       "14                                               MOD2  \n",
       "15                                               LIPA  \n",
       "16                                              PROS1  \n",
       "17                                              TGFB3  \n",
       "18                                                IDO  \n",
       "19                                                IDO  \n",
       "20                                                IDO  \n",
       "21                                                IDO  \n",
       "22                                       AAPK protein  \n",
       "23                                          AMPK gene  \n",
       "24                                       MTOR protein  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The below code reads each JSON files from the Workflows A through D (subdirectories). The queries are submitted to ARAX and output is saved in a dictionary, where the key is the file name of the JSON to denote which query is being run and the values assigned to the key is the query id**\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo/workflowA/A.0_RHOBTB2_direct.json\n",
      "A.0_RHOBTB2_direct\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='ars.transltr.io', port=443): Max retries exceeded with url: /ars/api/submit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11b480e80>: Failed to establish a new connection: [Errno 60] Operation timed out'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x11b480e80>: Failed to establish a new connection: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='ars.transltr.io', port=443): Max retries exceeded with url: /ars/api/submit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11b480e80>: Failed to establish a new connection: [Errno 60] Operation timed out'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2bad74193493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mkcresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_to_ars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b0944673bbf7>\u001b[0m in \u001b[0;36msubmit_to_ars\u001b[0;34m(m, ars_url, arax_url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubmit_to_ars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mars_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://ars.transltr.io/ars/api'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marax_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://arax.ncats.io'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msubmit_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{ars_url}/submit'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmit_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmessage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='ars.transltr.io', port=443): Max retries exceeded with url: /ars/api/submit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11b480e80>: Failed to establish a new connection: [Errno 60] Operation timed out'))"
     ]
    }
   ],
   "source": [
    "PATH = r'/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo'\n",
    "EXT = \"*.json\"\n",
    "dict_workflows = {}\n",
    "for root, dirs, files in os.walk(PATH): # step 1: accessing file\n",
    "    #print(root)\n",
    "    for name in files:\n",
    "        \n",
    "        if name.endswith((\".json\")):\n",
    "            if (name == 'C.1_Template_SmallMolecule_real_world_evidence_Disease.json') or (name == 'C.2_Template_Drug_Disease_GeneSet_interacts_with_SmallMolecule.json') or (name == 'C.3_Template_Disease_related_to_Drug.json'):\n",
    "                pass\n",
    "            else:\n",
    "                file_read = path.join(root, name)\n",
    "                dir_name = (os.path.splitext(os.path.basename(root))[0])\n",
    "                print(file_read)\n",
    "\n",
    "                filename = (os.path.splitext(os.path.basename(file_read))[0])\n",
    "                print(filename)\n",
    "                with open(file_read,'r') as inf:\n",
    "                    query = json.load(inf)\n",
    "\n",
    "                    kcresult = submit_to_ars(query)\n",
    "\n",
    "                    sleep(500)\n",
    "\n",
    "                    result_status = retrieve_ars_results(kcresult, name, check_sheet)\n",
    "\n",
    "\n",
    "                    dict_workflows[filename] = kcresult\n",
    "\n",
    "                    sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Codes below are for recording messages and generating outout as csv\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages = {}\n",
    "for keys, val in dict_workflows.items():\n",
    "    name = keys + '.json'\n",
    "    print(name, val)\n",
    "    \n",
    "\n",
    "    result_status = retrieve_ars_results(val, name, check_sheet)\n",
    "\n",
    "    workflow_result_messages[keys] = result_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe for workflows with PK\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert mesages to a dataframe\n",
    "col = []\n",
    "final_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    \n",
    "    col.append(k)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #count = 0\n",
    "    for key, value in workflow_result_messages[k][0].items():\n",
    "        \n",
    "        \n",
    "        #count= count+1\n",
    "        final_dict[key].append(value)\n",
    "\n",
    "    final_dict = dict(final_dict)\n",
    "    #print(count)\n",
    "df = pd.DataFrame(final_dict).T\n",
    "df.rename(columns=dict(zip(df.columns, col)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('ARS Error', 'No Results', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('{}','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating second table with edge attribute source\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict2 = defaultdict(dict)\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    col.append(k)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for key, value in workflow_result_messages[k][1].items():\n",
    "        final_dict2[k][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble = []\n",
    "for k, vs in final_dict2.items():\n",
    "    #print(k,vs)\n",
    "    for kv, v in vs.items():\n",
    "        for t in v:\n",
    "            final_dictassemble.append([k,kv,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Workflow', 'ARS-KPs', 'Values']\n",
    "df2 = pd.DataFrame(final_dictassemble, columns=column_names)\n",
    "df2 = df2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Values = df2.Values.apply(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2.groupby(['Workflow','Values'])['ARS-KPs'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = pd.DataFrame(df2test.unstack().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.drop([''], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.index = df2test.index.map(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.replace([], 'None', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test = df2test.mask(df2test.applymap(type).eq(list) & ~df2test.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting The index to push infores ID's that are compliant to Column L and M in INFORES Catalog\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/InfoRes Catalog - Translator InfoRes Catalog.csv\", header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[['id', 'name','translator category','has contributor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[:335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map = {}\n",
    "for i in df2test.index.values:\n",
    "    if i in infores_catalog['id'].values:\n",
    "        indices = infores_catalog[infores_catalog['id']==i].index[0]\n",
    "        if pd.notnull(infores_catalog.iloc[indices]['has contributor']):\n",
    "            dict_map[i] = infores_catalog.iloc[indices]['translator category']\n",
    "        else:\n",
    "            dict_map[i] = 'External Source'\n",
    "    else:\n",
    "        dict_map[i] = 'Illegal value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog']=df2test.index.map(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['sort']=pd.Categorical(df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog'], [\"KP\", \"ARA\",'External Source', 'Illegal Value'])\n",
    "df2test =df2test.sort_values(['sort'])\n",
    "df2test  = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df2test.columns.tolist()\n",
    "cols = [cols[-2]] + cols[:-2]\n",
    "df2test = df2test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['Query Type'] = 'Sync'\n",
    "\n",
    "df2test = df2test[['Query Type']+ list(df2test.columns[:-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Converting the Pk's to hyperlink\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['pk_id'] = df.loc['pk_id'].apply(lambda x: make_hyperlink(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'pk_id': 'pk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Highlight the cells In Excel\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(v):\n",
    "    if v.startswith('Results'):\n",
    "        return 'background-color: %s' % 'green'\n",
    "    elif v.startswith('Error'):\n",
    "        return 'background-color: %s' % 'red'\n",
    "    elif v.startswith('No Results'):\n",
    "        return 'background-color: %s' % 'yellow'\n",
    "    elif v.startswith('ARS Error'):\n",
    "        return 'background-color: %s' % 'blue'\n",
    "    elif v.startswith('Unknown'):\n",
    "        return 'background-color: %s' % 'magneta'\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled = df.style.applymap(highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "wks_name = 'Workflow Progress Tracker_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wks_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled.to_excel('/Users/priyash/Documents/GitHub/minihackathons/Notebooks/' + wks_name + '.xlsx')\n",
    "\n",
    "#styled.to_excel(writer, sheet_name='progress_tracker')\n",
    "#df2.to_excel(writer, sheet_name='edge_attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pushing dataframe to excel sheet on google drive\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Here I am using the google drive API to push the daatframe into an axcel sheet \n",
    "Every individula has the unique credential file that they need to create for google drive API -- \n",
    "\"araxworkflowprogresstesting-2632632db8be.json\" -- is the credential used from my drive. place this json file where\n",
    "the ReadAndRunAllWorkFLows.ipynb will be. NB: i have removed my credntial file for privacy reasons. Always remove\n",
    "the json file before making committs to the repo. To use googe Drive API follow: https://towardsdatascience.com/how-to-manage-files-in-google-drive-with-python-d26471d91ecd**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Query Type'] = 'Sync'\n",
    "\n",
    "df = df[['Query Type']+ list(df.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.rename({'index': 'ARA/KP'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Google Drive API\n",
    "\n",
    "**Push the dataframe to a google sheet via google drive API and then format the google spread sheet to add hyperlink to pk's and color the cells\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 1\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from gspread_formatting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    'araxworkflowprogresstesting-2632632db8be.json', scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "d2g.upload(df, spreadsheet_key, wks_name, credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 2\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wks2 = 'edge_attribute_source_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "d2g.upload(df2test, spreadsheet_key, wks2, credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

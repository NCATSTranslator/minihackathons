{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Query Status in CSV for all Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Read all the JSON files for all the workflows and print out the messages and query status to a CSV file**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules. NB: submit_run_ars_modules contains all the modules to submit job to ARAX\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from gamma_viewer import GammaViewer\n",
    "from IPython.display import display\n",
    "#from submit_run_ars_modules import submit_to_ars, submit_to_devars, printjson, retrieve_devars_results\n",
    "import glob \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_ars(m,ars_url='https://ars.transltr.io/ars/api',arax_url='https://arax.ncats.io'):\n",
    "    submit_url=f'{ars_url}/submit'\n",
    "    response = requests.post(submit_url,json=m)\n",
    "    try:\n",
    "        message_id = response.json()['pk']\n",
    "    except:\n",
    "        print('fail')\n",
    "        message_id = None\n",
    "    print(f'{arax_url}/?source=ARS&id={message_id}')\n",
    "    return message_id\n",
    "\n",
    "##https://ars.ci.transltr.io/ars/api\n",
    "\n",
    "def retrieve_ars_results(mid, name, check_sheet, ars_url='https://ars.transltr.io/ars/api'):\n",
    "    pk = 'https://arax.ncats.io/?source=ARS&id=' + mid\n",
    "    message_url = f'{ars_url}/messages/{mid}?trace=y'\n",
    "    response = requests.get(message_url)\n",
    "    j = response.json()\n",
    "    print( j['status'] )\n",
    "    results = {}\n",
    "    dictionary = {}\n",
    "    dictionary_2 = {}\n",
    "    dict3 = {}\n",
    "    for child in j['children']:\n",
    "        print(child['status'])\n",
    "        error_code = child['code']\n",
    "        \n",
    "        if child['status']  == 'Done':\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                nresults = len(child_response['fields']['data']['message']['results'])\n",
    "                if nresults > 0:\n",
    "                    results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                if name in list(check_sheet.Workflow):\n",
    "                    print(name)\n",
    "                    dfy = check_sheet[check_sheet['Workflow']== name]\n",
    "            \n",
    "                    dfy.reset_index(drop=True)\n",
    "\n",
    "                    for index,curie_id in enumerate(dfy.Curie):\n",
    "                        print(index,curie_id)\n",
    "                        node_num = dfy.iloc[index][3]\n",
    "                        query_id = curie_id\n",
    "                        if np.isnan(dfy.iloc[index][2]) == True:\n",
    "                            len_check = len(child_response['fields']['data']['message']['results'])\n",
    "                        else:\n",
    "                            len_check = dfy.iloc[index][2]\n",
    "\n",
    "                        #print(node_num, query_id, len_check)\n",
    "\n",
    "                        locs = []\n",
    "                        for x, val in enumerate(child_response['fields']['data']['message']['results']):\n",
    "                            #print(val)\n",
    "\n",
    "                            if x < len_check:\n",
    "\n",
    "                                if query_id in val['node_bindings'][node_num][0]['id']:\n",
    "                                    locs.append(x)\n",
    "                        if not locs:\n",
    "                            check_result = f'False'\n",
    "                            print(check_result)\n",
    "                            #pass\n",
    "                        else:\n",
    "                            check_result = f'True'\n",
    "                            print('curie id:', query_id, ': INCLUDED at postion N ==', locs, 'on', node_num)\n",
    "\n",
    "                        dict3[curie_id] = check_result    \n",
    "                    \n",
    "                if child_response['fields']['data']['message']['knowledge_graph']['edges']:\n",
    "                    if child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                            edge_ex = child_response['fields']['data']['message']['knowledge_graph']['edges']\n",
    "                            test_att_values =[]\n",
    "                            for val in child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                                #print(val)\n",
    "                                \n",
    "                                for tx in edge_ex[val]['attributes']:\n",
    "                                    if (tx['attribute_type_id'] == 'biolink:primary_knowledge_source') or (tx['attribute_type_id'] == 'biolink:original_knowledge_source') or (tx['attribute_type_id'] == 'biolink:aggregator_knowledge_source') :\n",
    "                                        \n",
    "                                        \n",
    "                                        value_att = tx['value']\n",
    "                        \n",
    "                                        test_att_values.append(value_att)\n",
    "                                        test_att = set(flatten_list(test_att_values))\n",
    "                                        \n",
    "                                        \n",
    "                                        dictionary_2[child['actor']['agent']] = test_att\n",
    "                    #else:\n",
    "                        #dictionary_2[child['actor']['agent']] = [] \n",
    "                #else:\n",
    "                   # dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "            except Exception as e:\n",
    "                nresults=0\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "                \n",
    "            \n",
    "        \n",
    "        elif child['status'] == 'Error':\n",
    "            nresults=0\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            nresults = 0\n",
    "            #dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "        dictionary['pk_id'] =  pk  \n",
    "            \n",
    "        if ((child['status'] == 'Done') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'No Results' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'No Results']\n",
    "        elif ((child['status'] == 'ARS Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'ARS Error' ': ' + str(error_code)\n",
    "        elif ((child['status'] == 'Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'Error' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'ARS Error']\n",
    "        elif ((child['status'] == 'Done') & (nresults != 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Results' ': ' + str(error_code) + str(dict3)\n",
    "        elif ((child['status'] == 'Unknown') & (nresults == 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Unknown' ': ' + str(error_code)\n",
    "        \n",
    "        \n",
    "        print(child['actor']['agent'], child['status'], nresults)\n",
    "        #test =  [child['actor']['agent'], child['status'], nresults]\n",
    "        #test2.append(test)\n",
    "    return [dictionary, dictionary_2]\n",
    "\n",
    "\n",
    "#def submit_to_devars(m):\n",
    "#    return submit_to_ars(m,ars_url='https://ars-dev.transltr.io/ars/api',arax_url='https://arax.ncats.io')\n",
    "\n",
    "#def retrieve_devars_results(m):\n",
    "#     return retrieve_ars_results(m,ars_url='https://ars-dev.transltr.io/ars/api')\n",
    "\n",
    "def printjson(j):\n",
    "    print(json.dumps(j,indent=4))\n",
    "    \n",
    "def make_hyperlink(value):\n",
    "    return '=HYPERLINK(\"%s\", \"%s\")' % (value.format(value), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sheet = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/Query results to include or exclude - Sheet1.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Workflow</th>\n",
       "      <th>N (size of list of results)</th>\n",
       "      <th>Query node ID</th>\n",
       "      <th>Include/Exclude</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.0_RHOBTB2_direct.json</td>\n",
       "      <td>PUBCHEM.COMPOUND:2950270</td>\n",
       "      <td>10.0</td>\n",
       "      <td>n1</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Note: this example row means that for workflow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B.1_DILI-three-hop-from-MONDO:0005359_DILI.json</td>\n",
       "      <td>RXNORM:155067</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B.1_DILI_three-hop-from-SNOMEDCT:197354009_Tox...</td>\n",
       "      <td>CHEMBL.COMPOUND:CHEMBL1698267</td>\n",
       "      <td>500.0</td>\n",
       "      <td>n3</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equivalent identifiers are equally acceptable;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D.1_parkinsons-crohns.json</td>\n",
       "      <td>NCBIGene:120892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LRRK2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D.1_parkinsons-crohns.json</td>\n",
       "      <td>NCBIGene:11315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PARK7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D.1_parkinsons-crohns.json</td>\n",
       "      <td>NCBIGene:110357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOD2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D.2_ssri-heart-disease.json</td>\n",
       "      <td>NCBIGene:3988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D.2_ssri-heart-disease.json</td>\n",
       "      <td>NCBIGene:5627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D.2_ssri-heart-disease.json</td>\n",
       "      <td>NCBIGene:7043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TGFB3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D.3_ssri-heart-disease-one-hop.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e00</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>any clinical edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>REACT:R-HSA-888614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n02</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>KEGG:1.13.11.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n02</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>REACT:R-HSA-888614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n03</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D.4_tryptophan-kyurenine.json</td>\n",
       "      <td>KEGG:1.13.11.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n03</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D.6_metformin-ferritin.json</td>\n",
       "      <td>UniProtKB:P54646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPK protein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D.6_metformin-ferritin.json</td>\n",
       "      <td>NCBIGene:5563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n01</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMPK gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D.6_metformin-ferritin.json</td>\n",
       "      <td>UniProtKB:P42345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n02</td>\n",
       "      <td>include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MTOR protein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Unnamed: 0  \\\n",
       "0                             A.0_RHOBTB2_direct.json   \n",
       "1     B.1_DILI-three-hop-from-MONDO:0005359_DILI.json   \n",
       "2   B.1_DILI_three-hop-from-SNOMEDCT:197354009_Tox...   \n",
       "3                          D.1_parkinsons-crohns.json   \n",
       "4                          D.1_parkinsons-crohns.json   \n",
       "5                          D.1_parkinsons-crohns.json   \n",
       "6                         D.2_ssri-heart-disease.json   \n",
       "7                         D.2_ssri-heart-disease.json   \n",
       "8                         D.2_ssri-heart-disease.json   \n",
       "9                 D.3_ssri-heart-disease-one-hop.json   \n",
       "10                      D.4_tryptophan-kyurenine.json   \n",
       "11                      D.4_tryptophan-kyurenine.json   \n",
       "12                      D.4_tryptophan-kyurenine.json   \n",
       "13                      D.4_tryptophan-kyurenine.json   \n",
       "14                        D.6_metformin-ferritin.json   \n",
       "15                        D.6_metformin-ferritin.json   \n",
       "16                        D.6_metformin-ferritin.json   \n",
       "\n",
       "                         Workflow  N (size of list of results) Query node ID  \\\n",
       "0        PUBCHEM.COMPOUND:2950270                         10.0            n1   \n",
       "1                   RXNORM:155067                        500.0            n3   \n",
       "2   CHEMBL.COMPOUND:CHEMBL1698267                        500.0            n3   \n",
       "3                 NCBIGene:120892                          NaN           n01   \n",
       "4                  NCBIGene:11315                          NaN           n01   \n",
       "5                 NCBIGene:110357                          NaN           n01   \n",
       "6                   NCBIGene:3988                          NaN           n01   \n",
       "7                   NCBIGene:5627                          NaN           n01   \n",
       "8                   NCBIGene:7043                          NaN           n01   \n",
       "9                             NaN                          NaN           e00   \n",
       "10             REACT:R-HSA-888614                          NaN           n02   \n",
       "11                KEGG:1.13.11.52                          NaN           n02   \n",
       "12             REACT:R-HSA-888614                          NaN           n03   \n",
       "13                KEGG:1.13.11.52                          NaN           n03   \n",
       "14               UniProtKB:P54646                          NaN           n01   \n",
       "15                  NCBIGene:5563                          NaN           n01   \n",
       "16               UniProtKB:P42345                          NaN           n02   \n",
       "\n",
       "   Include/Exclude  Unnamed: 5  \\\n",
       "0          Include         NaN   \n",
       "1          Include         NaN   \n",
       "2          Include         NaN   \n",
       "3          include         NaN   \n",
       "4          include         NaN   \n",
       "5          include         NaN   \n",
       "6          include         NaN   \n",
       "7          include         NaN   \n",
       "8          include         NaN   \n",
       "9          include         NaN   \n",
       "10         include         NaN   \n",
       "11         include         NaN   \n",
       "12         include         NaN   \n",
       "13         include         NaN   \n",
       "14         include         NaN   \n",
       "15         include         NaN   \n",
       "16         include         NaN   \n",
       "\n",
       "                                           Unnamed: 6  \n",
       "0   Note: this example row means that for workflow...  \n",
       "1       Equivalent identifiers are equally acceptable  \n",
       "2   Equivalent identifiers are equally acceptable;...  \n",
       "3                                               LRRK2  \n",
       "4                                               PARK7  \n",
       "5                                                MOD2  \n",
       "6                                                LIPA  \n",
       "7                                               PROS1  \n",
       "8                                               TGFB3  \n",
       "9                                   any clinical edge  \n",
       "10                                                IDO  \n",
       "11                                                IDO  \n",
       "12                                                IDO  \n",
       "13                                                IDO  \n",
       "14                                       AAPK protein  \n",
       "15                                          AMPK gene  \n",
       "16                                       MTOR protein  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The below code reads each JSON files from the Workflows A through D (subdirectories). The queries are submitted to ARAX and output is saved in a dictionary, where the key is the file name of the JSON to denote which query is being run and the values assigned to the key is the query id**\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo/workflowA/A.3_KCNMA1.json\n",
      "A.3_KCNMA1\n",
      "https://arax.ncats.io/?source=ARS&id=ebcaa641-a59f-4b62-a588-93781cd6e5cf\n",
      "Done\n",
      "Done\n",
      "ara-improving Done 5\n",
      "Error\n",
      "ara-aragorn-exp Error 0\n",
      "Done\n",
      "kp-icees-dili Done 0\n",
      "Done\n",
      "kp-chp Done 30\n",
      "Error\n",
      "ara-robokop Error 0\n",
      "Done\n",
      "ara-arax Done 163\n",
      "Done\n",
      "kp-icees Done 0\n",
      "Done\n",
      "kp-cohd ARS Error 0\n",
      "Done\n",
      "ara-bte Done 69\n",
      "Unknown\n",
      "ara-ncats Unknown 0\n",
      "Done\n",
      "ara-unsecret Done 102\n",
      "Done\n",
      "kp-openpredict Done 0\n",
      "Done\n",
      "kp-genetics Done 0\n",
      "Done\n",
      "kp-molecular Done 99\n",
      "Done\n",
      "ara-explanatory Done 0\n",
      "Done\n",
      "kp-cam Done 0\n",
      "Done\n",
      "kp-textmining Done 0\n",
      "Done\n",
      "ara-aragorn Done 951\n",
      "/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo/workflowA/A.2a_RHOBTB2_twohop.json\n",
      "A.2a_RHOBTB2_twohop\n",
      "https://arax.ncats.io/?source=ARS&id=28bbeaae-1db9-407a-ab1c-378667eee080\n"
     ]
    }
   ],
   "source": [
    "PATH = r'/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo'\n",
    "EXT = \"*.json\"\n",
    "dict_workflows = {}\n",
    "for root, dirs, files in os.walk(PATH): # step 1: accessing file\n",
    "    #print(root)\n",
    "    for name in files:\n",
    "        \n",
    "        if name.endswith((\".json\")):\n",
    "            if (name == 'C.2c_Natalizumab_MultSclerosis_GeneSet_and_SmallMolecule.json') or (name == 'C.2b_Etanercept_MultSclerosis_GeneSet_and_SmallMolecule.json') or (name == 'C.2a_Imatinib_MultSclerosis_GeneSet_and_SmallMolecule.json') or (name == 'C.1_Template_SmallMolecule_real_world_evidence_Disease.json') or (name == 'C.2_Template_Drug_Disease_GeneSet_interacts_with_SmallMolecule.json') or (name == 'C.3_Template_Disease_related_to_Drug.json'):\n",
    "                pass\n",
    "            else:\n",
    "                file_read = path.join(root, name)\n",
    "                dir_name = (os.path.splitext(os.path.basename(root))[0])\n",
    "                print(file_read)\n",
    "\n",
    "                filename = (os.path.splitext(os.path.basename(file_read))[0])\n",
    "                print(filename)\n",
    "                with open(file_read,'r') as inf:\n",
    "                    query = json.load(inf)\n",
    "\n",
    "                    kcresult = submit_to_ars(query)\n",
    "\n",
    "                    sleep(500)\n",
    "\n",
    "                    result_status = retrieve_ars_results(kcresult, name, check_sheet)\n",
    "\n",
    "\n",
    "                    dict_workflows[filename] = kcresult\n",
    "\n",
    "                    sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Codes below are for recording messages and generating outout as csv\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages = {}\n",
    "for keys, val in dict_workflows.items():\n",
    "    name = keys + '.json'\n",
    "    print(name, val)\n",
    "    \n",
    "    result_status = retrieve_ars_results(val, name, check_sheet)\n",
    "    \n",
    "    workflow_result_messages[keys] = result_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe for workflows with PK\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert mesages to a dataframe\n",
    "col = []\n",
    "final_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    \n",
    "    if (k == 'C.2b_Etanercept_MultSclerosis_GeneSet_and_SmallMolecule') or (k == 'C.2c_Natalizumab_MultSclerosis_GeneSet_and_SmallMolecule') or (k == 'C.2a_Imatinib_MultSclerosis_GeneSet_and_SmallMolecule'):\n",
    "            pass\n",
    "    else:\n",
    "    \n",
    "        col.append(k)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #count = 0\n",
    "        for key, value in workflow_result_messages[k][0].items():\n",
    "        \n",
    "        \n",
    "        #count= count+1\n",
    "            final_dict[key].append(value)\n",
    "\n",
    "    final_dict = dict(final_dict)\n",
    "    #print(count)\n",
    "df = pd.DataFrame(final_dict).T\n",
    "df.rename(columns=dict(zip(df.columns, col)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('ARS Error', 'No Results', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('{}','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating second table with edge attribute source\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict2 = defaultdict(dict)\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    col.append(k)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for key, value in workflow_result_messages[k][1].items():\n",
    "        final_dict2[k][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble = []\n",
    "for k, vs in final_dict2.items():\n",
    "    #print(k,vs)\n",
    "    for kv, v in vs.items():\n",
    "        for t in v:\n",
    "            final_dictassemble.append([k,kv,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Workflow', 'ARS-KPs', 'Values']\n",
    "df2 = pd.DataFrame(final_dictassemble, columns=column_names)\n",
    "df2 = df2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Values = df2.Values.apply(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2.groupby(['Workflow','Values'])['ARS-KPs'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = pd.DataFrame(df2test.unstack().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.drop([''], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.index = df2test.index.map(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.replace([], 'None', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test = df2test.mask(df2test.applymap(type).eq(list) & ~df2test.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting The index to push infores ID's that are compliant to Column L and M in INFORES Catalog\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/InfoRes Catalog - Translator InfoRes Catalog.csv\", header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[['id', 'name','translator category','has contributor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[:335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map = {}\n",
    "for i in df2test.index.values:\n",
    "    if i in infores_catalog['id'].values:\n",
    "        indices = infores_catalog[infores_catalog['id']==i].index[0]\n",
    "        if pd.notnull(infores_catalog.iloc[indices]['has contributor']):\n",
    "            dict_map[i] = infores_catalog.iloc[indices]['translator category']\n",
    "        else:\n",
    "            dict_map[i] = 'External Source'\n",
    "    else:\n",
    "        dict_map[i] = 'Illegal value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog']=df2test.index.map(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['sort']=pd.Categorical(df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog'], [\"KP\", \"ARA\",'External Source', 'Illegal Value'])\n",
    "df2test =df2test.sort_values(['sort'])\n",
    "df2test  = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df2test.columns.tolist()\n",
    "cols = [cols[-2]] + cols[:-2]\n",
    "df2test = df2test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['Query Type'] = 'Sync'\n",
    "\n",
    "df2test = df2test[['Query Type']+ list(df2test.columns[:-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Converting the Pk's to hyperlink\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['pk_id'] = df.loc['pk_id'].apply(lambda x: make_hyperlink(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'pk_id': 'pk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Highlight the cells In Excel\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(v):\n",
    "    if v.startswith('Results'):\n",
    "        return 'background-color: %s' % 'green'\n",
    "    elif v.startswith('Error'):\n",
    "        return 'background-color: %s' % 'red'\n",
    "    elif v.startswith('No Results'):\n",
    "        return 'background-color: %s' % 'yellow'\n",
    "    elif v.startswith('ARS Error'):\n",
    "        return 'background-color: %s' % 'blue'\n",
    "    elif v.startswith('Unknown'):\n",
    "        return 'background-color: %s' % 'magneta'\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled = df.style.applymap(highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "wks_name = 'Workflow Progress Tracker_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wks_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled.to_excel('/Users/priyash/Documents/GitHub/minihackathons/Notebooks/' + wks_name + '.xlsx')\n",
    "\n",
    "#styled.to_excel(writer, sheet_name='progress_tracker')\n",
    "#df2.to_excel(writer, sheet_name='edge_attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pushing dataframe to excel sheet on google drive\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Here I am using the google drive API to push the daatframe into an axcel sheet \n",
    "Every individula has the unique credential file that they need to create for google drive API -- \n",
    "\"araxworkflowprogresstesting-2632632db8be.json\" -- is the credential used from my drive. place this json file where\n",
    "the ReadAndRunAllWorkFLows.ipynb will be. NB: i have removed my credntial file for privacy reasons. Always remove\n",
    "the json file before making committs to the repo. To use googe Drive API follow: https://towardsdatascience.com/how-to-manage-files-in-google-drive-with-python-d26471d91ecd**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Query Type'] = 'Sync'\n",
    "\n",
    "df = df[['Query Type']+ list(df.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.rename({'index': 'ARA/KP'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Google Drive API\n",
    "\n",
    "**Push the dataframe to a google sheet via google drive API and then format the google spread sheet to add hyperlink to pk's and color the cells\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 1\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from gspread_formatting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    'araxworkflowprogresstesting-2632632db8be.json', scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "d2g.upload(df, spreadsheet_key, wks_name, credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 2\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wks2 = 'edge_attribute_source_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "d2g.upload(df2test, spreadsheet_key, wks2, credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

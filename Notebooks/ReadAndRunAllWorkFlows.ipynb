{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Query Status in CSV for all Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Read all the JSON files for all the workflows and print out the messages and query status to a CSV file**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules. NB: submit_run_ars_modules contains all the modules to submit job to ARAX\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from gamma_viewer import GammaViewer\n",
    "from IPython.display import display\n",
    "#from submit_run_ars_modules import submit_to_ars, submit_to_devars, printjson, retrieve_devars_results\n",
    "import glob \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_ars(m,ars_url='https://ars.transltr.io/ars/api',arax_url='https://arax.ncats.io'):\n",
    "    submit_url=f'{ars_url}/submit'\n",
    "    response = requests.post(submit_url,json=m)\n",
    "    try:\n",
    "        message_id = response.json()['pk']\n",
    "    except:\n",
    "        print('fail')\n",
    "        message_id = None\n",
    "    print(f'{arax_url}/?source=ARS&id={message_id}')\n",
    "    return message_id\n",
    "\n",
    "##https://ars.ci.transltr.io/ars/api\n",
    "\n",
    "def retrieve_ars_results(mid, name, check_sheet, ars_url='https://ars.transltr.io/ars/api'):\n",
    "    pk = 'https://arax.ncats.io/?source=ARS&id=' + mid\n",
    "    message_url = f'{ars_url}/messages/{mid}?trace=y'\n",
    "    response = requests.get(message_url)\n",
    "    j = response.json()\n",
    "    print( j['status'] )\n",
    "    results = {}\n",
    "    dictionary = {}\n",
    "    dictionary_2 = {}\n",
    "    dict3 = {}\n",
    "    for child in j['children']:\n",
    "        print(child['status'])\n",
    "        error_code = child['code']\n",
    "        \n",
    "        if child['status']  == 'Done':\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                nresults = len(child_response['fields']['data']['message']['results'])\n",
    "                if nresults > 0:\n",
    "                    results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                    \n",
    "                    if name in list(check_sheet.Workflow):\n",
    "                        print(name)\n",
    "                        dfy = check_sheet[check_sheet['Workflow']== name]\n",
    "\n",
    "                        dfy.reset_index(drop=True)\n",
    "\n",
    "                        for index,curie_id in enumerate(dfy.Curie):\n",
    "                            print(index,curie_id)\n",
    "                            node_num = dfy.iloc[index][3]\n",
    "                            query_id = curie_id\n",
    "                            if np.isnan(dfy.iloc[index][2]) == True:\n",
    "                                len_check = len(child_response['fields']['data']['message']['results'])\n",
    "                                len_check = int(len_check)\n",
    "                            else:\n",
    "                                len_check = int(dfy.iloc[index][2])\n",
    "\n",
    "                            #print(node_num, query_id, len_check)\n",
    "\n",
    "                            locs = []\n",
    "                            for x, val in enumerate(child_response['fields']['data']['message']['results']):\n",
    "                                #print(val)\n",
    "\n",
    "                                if x < len_check:\n",
    "\n",
    "                                    if query_id in val['node_bindings'][node_num][0]['id']:\n",
    "                                        locs.append(x)\n",
    "                            if not locs:\n",
    "                                check_result = f'False'\n",
    "                                print(check_result)\n",
    "                                #pass\n",
    "                            else:\n",
    "                                check_result = f'True'\n",
    "                                print('curie id:', query_id, ': INCLUDED at postion N ==', locs, 'on', node_num)\n",
    "\n",
    "                            dict3[curie_id] = check_result    \n",
    "\n",
    "                if child_response['fields']['data']['message']['knowledge_graph']['edges']:\n",
    "                    if child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                            edge_ex = child_response['fields']['data']['message']['knowledge_graph']['edges']\n",
    "                            test_att_values =[]\n",
    "                            for val in child_response['fields']['data']['message']['knowledge_graph']['edges'].keys():\n",
    "                                #print(val)\n",
    "                                \n",
    "                                for tx in edge_ex[val]['attributes']:\n",
    "                                    if (tx['attribute_type_id'] == 'biolink:primary_knowledge_source') or (tx['attribute_type_id'] == 'biolink:original_knowledge_source') or (tx['attribute_type_id'] == 'biolink:aggregator_knowledge_source') :\n",
    "                                        \n",
    "                                        \n",
    "                                        value_att = tx['value']\n",
    "                        \n",
    "                                        test_att_values.append(value_att)\n",
    "                                        test_att = set(flatten_list(test_att_values))\n",
    "                                        \n",
    "                                        \n",
    "                                        dictionary_2[child['actor']['agent']] = test_att\n",
    "                    #else:\n",
    "                        #dictionary_2[child['actor']['agent']] = [] \n",
    "                #else:\n",
    "                   # dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "            except Exception as e:\n",
    "                nresults=0\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "                \n",
    "            \n",
    "        \n",
    "        elif child['status'] == 'Error':\n",
    "            nresults=0\n",
    "            childmessage_id = child['message']\n",
    "            child_url = f'{ars_url}/messages/{childmessage_id}'\n",
    "            try:\n",
    "                child_response = requests.get(child_url).json()\n",
    "                results[child['actor']['agent']] = {'message':child_response['fields']['data']['message']}\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                child['status'] = 'ARS Error'\n",
    "                #dictionary_2[child['actor']['agent']] = []\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            nresults = 0\n",
    "            #dictionary_2[child['actor']['agent']] = []\n",
    "            \n",
    "        dictionary['pk_id'] =  pk  \n",
    "            \n",
    "        if ((child['status'] == 'Done') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'No Results' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'No Results']\n",
    "        elif ((child['status'] == 'ARS Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'ARS Error' ': ' + str(error_code)\n",
    "        elif ((child['status'] == 'Error') & (nresults == 0)):\n",
    "            dictionary[child['actor']['agent']] = 'Error' ': ' + str(error_code)\n",
    "            #test =  [child['actor']['agent'], 'ARS Error']\n",
    "        elif ((child['status'] == 'Done') & (nresults != 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Results' ': ' + str(error_code) + ' ' + str(dict3)\n",
    "        elif ((child['status'] == 'Unknown') & (nresults == 0)):\n",
    "            #test =  [child['actor']['agent'], 'Results']\n",
    "            dictionary[child['actor']['agent']] = 'Unknown' ': ' + str(error_code)\n",
    "        else:\n",
    "            dictionary[child['actor']['agent']] = 'Running'\n",
    "            \n",
    "        \n",
    "        \n",
    "        print(child['actor']['agent'], child['status'], nresults)\n",
    "        #test =  [child['actor']['agent'], child['status'], nresults]\n",
    "        #test2.append(test)\n",
    "    return [dictionary, dictionary_2]\n",
    "\n",
    "\n",
    "#def submit_to_devars(m):\n",
    "#    return submit_to_ars(m,ars_url='https://ars-dev.transltr.io/ars/api',arax_url='https://arax.ncats.io')\n",
    "\n",
    "#def retrieve_devars_results(m):\n",
    "#     return retrieve_ars_results(m,ars_url='https://ars-dev.transltr.io/ars/api')\n",
    "\n",
    "def printjson(j):\n",
    "    print(json.dumps(j,indent=4))\n",
    "    \n",
    "def make_hyperlink(value):\n",
    "    return '=HYPERLINK(\"%s\", \"%s\")' % (value.format(value), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sheet = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/Query results to include or exclude - Sheet1.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The below code reads each JSON files from the Workflows A through D (subdirectories). The queries are submitted to ARAX and output is saved in a dictionary, where the key is the file name of the JSON to denote which query is being run and the values assigned to the key is the query id**\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = r'/Users/priyash/Documents/GitHub/minihackathons/2021-12_demo'\n",
    "EXT = \"*.json\"\n",
    "dict_workflows = {}\n",
    "for root, dirs, files in os.walk(PATH): # step 1: accessing file\n",
    "    #print(root)\n",
    "    for name in files:\n",
    "        \n",
    "        if name.endswith((\".json\")):\n",
    "            if (name == 'C.1_Template_SmallMolecule_real_world_evidence_Disease.json') or (name == 'C.2_Template_Drug_Disease_GeneSet_interacts_with_SmallMolecule.json') or (name == 'C.3_Template_Disease_related_to_Drug.json'):\n",
    "                pass\n",
    "            else:\n",
    "                file_read = path.join(root, name)\n",
    "                dir_name = (os.path.splitext(os.path.basename(root))[0])\n",
    "                print(file_read)\n",
    "\n",
    "                filename = (os.path.splitext(os.path.basename(file_read))[0])\n",
    "                print(filename)\n",
    "                with open(file_read,'r') as inf:\n",
    "                    query = json.load(inf)\n",
    "\n",
    "                    kcresult = submit_to_ars(query)\n",
    "\n",
    "                    #sleep(600)\n",
    "\n",
    "                    #result_status = retrieve_ars_results(kcresult, name, check_sheet)\n",
    "\n",
    "\n",
    "                    dict_workflows[filename] = kcresult\n",
    "\n",
    "                    sleep(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Codes below are for recording messages and generating outout as csv\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sleep(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workflow_result_messages = {}\n",
    "for keys, val in dict_workflows.items():\n",
    "    name = keys + '.json'\n",
    "    print(name, val)\n",
    "    \n",
    "\n",
    "    result_status = retrieve_ars_results(val, name, check_sheet)\n",
    "\n",
    "    workflow_result_messages[keys] = result_status\n",
    "    \n",
    "    sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe for workflows with PK\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Convert mesages to a dataframe\n",
    "col = []\n",
    "final_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    col.append(k)\n",
    "\n",
    "    count = 0\n",
    "    for key, value in workflow_result_messages[k][0].items():\n",
    "        \n",
    "        \n",
    "        count= count+1\n",
    "        final_dict[key].append(value)\n",
    "\n",
    "    final_dict = dict(final_dict)\n",
    "    print(count)\n",
    "df = pd.DataFrame(final_dict).T\n",
    "df.rename(columns=dict(zip(df.columns, col)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages['B.2b_DILI-fourth-one-hop-from-MESH_D000077185_Resveratrol'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_result_messages['B.2b_DILI-fourth-one-hop-from-MESH_D000077185_Resveratrol'][0].update({'ara-bte': 'running'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('ARS Error', 'No Results', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('{}','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating second table with edge attribute source\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict2 = defaultdict(dict)\n",
    "for k in sorted(workflow_result_messages):\n",
    "    print(k)\n",
    "    col.append(k)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for key, value in workflow_result_messages[k][1].items():\n",
    "        final_dict2[k][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble = []\n",
    "for k, vs in final_dict2.items():\n",
    "    #print(k,vs)\n",
    "    for kv, v in vs.items():\n",
    "        for t in v:\n",
    "            final_dictassemble.append([k,kv,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictassemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Workflow', 'ARS-KPs', 'Values']\n",
    "df2 = pd.DataFrame(final_dictassemble, columns=column_names)\n",
    "df2 = df2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Values = df2.Values.apply(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2.groupby(['Workflow','Values'])['ARS-KPs'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = pd.DataFrame(df2test.unstack().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.drop([''], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test.index = df2test.index.map(lambda x: x[2:-2] if ('[' in x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.replace([], 'None', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test = df2test.mask(df2test.applymap(type).eq(list) & ~df2test.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting The index to push infores ID's that are compliant to Column L and M in INFORES Catalog\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = pd.read_csv(\"/Users/priyash/Documents/GitHub/minihackathons/Notebooks/InfoRes Catalog - Translator InfoRes Catalog.csv\", header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[['id', 'name','translator category','has contributor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infores_catalog = infores_catalog[:337]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map = {}\n",
    "for i in df2test.index.values:\n",
    "    if i in infores_catalog['id'].values:\n",
    "        indices = infores_catalog[infores_catalog['id']==i].index[0]\n",
    "        if pd.notnull(infores_catalog.iloc[indices]['has contributor']):\n",
    "            dict_map[i] = infores_catalog.iloc[indices]['translator category']\n",
    "        else:\n",
    "            dict_map[i] = 'External Source'\n",
    "    else:\n",
    "        dict_map[i] = 'Illegal value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog']=df2test.index.map(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test['sort']=pd.Categorical(df2test['Translator_Category_Complaint_to_ColL&M_InforesCatalog'], [\"KP\", \"ARA\",'External Source', 'Illegal Value'])\n",
    "df2test =df2test.sort_values(['sort'])\n",
    "df2test  = df2test.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df2test.columns.tolist()\n",
    "cols = [cols[-2]] + cols[:-2]\n",
    "df2test = df2test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2test['Query Type'] = 'Sync'\n",
    "\n",
    "#df2test = df2test[['Query Type']+ list(df2test.columns[:-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Converting the Pk's to hyperlink\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['pk_id'] = df.loc['pk_id'].apply(lambda x: make_hyperlink(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'pk_id': 'pk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Highlight the cells In Excel\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(v):\n",
    "    if v.startswith('Results'):\n",
    "        return 'background-color: %s' % 'green'\n",
    "    elif v.startswith('Error'):\n",
    "        return 'background-color: %s' % 'red'\n",
    "    elif v.startswith('No Results'):\n",
    "        return 'background-color: %s' % 'yellow'\n",
    "    elif v.startswith('ARS Error'):\n",
    "        return 'background-color: %s' % 'blue'\n",
    "    elif v.startswith('Unknown'):\n",
    "        return 'background-color: %s' % 'magneta'\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled = df.style.applymap(highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "wks_name = 'Workflow Progress Tracker_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled.to_excel('/Users/priyash/Documents/GitHub/minihackathons/Notebooks/' + wks_name + '.xlsx')\n",
    "\n",
    "#styled.to_excel(writer, sheet_name='progress_tracker')\n",
    "#df2.to_excel(writer, sheet_name='edge_attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pushing dataframe to excel sheet on google drive\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Here I am using the google drive API to push the daatframe into an axcel sheet \n",
    "Every individula has the unique credential file that they need to create for google drive API -- \n",
    "\"araxworkflowprogresstesting-2632632db8be.json\" -- is the credential used from my drive. place this json file where\n",
    "the ReadAndRunAllWorkFLows.ipynb will be. NB: i have removed my credntial file for privacy reasons. Always remove\n",
    "the json file before making committs to the repo. To use googe Drive API follow: https://towardsdatascience.com/how-to-manage-files-in-google-drive-with-python-d26471d91ecd**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Query Type'] = 'Sync'\n",
    "\n",
    "#df = df[['Query Type']+ list(df.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.rename({'index': 'ARA/KP'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Google Drive API\n",
    "\n",
    "**Push the dataframe to a google sheet via google drive API and then format the google spread sheet to add hyperlink to pk's and color the cells\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 1\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from gspread_formatting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    'araxworkflowprogresstesting-2632632db8be.json', scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "d2g.upload(df, spreadsheet_key, wks_name, credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Dataframe 2\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wks2 = 'edge_attribute_source_' + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1O1cMmYGxoIqP6xbzj6FG5owiKQVg57wx2O_XIA_hN_A'\n",
    "#spreadsheet_key = '1sPpBIkxrHbQNiTm5oPs9-5KrjsyXcgaVAxknJj-u8pY'\n",
    "#wks_name = 'Workflow Progress Tracker_' + date\n",
    "d2g.upload(df2test, spreadsheet_key, wks2, credentials=credentials, row_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_frozen(ws2, cols=1, rows =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
